# 138 Linear Basis models
* Equation 3.3 with the bias parameter in the $\Sigma$: $\Phi_0(X) = 1$
  * X vector of diemension D
  * y (regression) of dimension 1
# 139
* eq 36 Sigmo√Ød function see graph at [Wikipedia](https://en.wikipedia.org/wiki/Sigmoid_function)
# 140
* Equation 3.9 p 140 was given (like written in RED on the left margin) by Equation 1.89 in Chapter 1 (p 47)
* [Unimodal](https://en.wikipedia.org/wiki/Unimodality) for a distribution means which has a single Peak
* [Multivariate t](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) if each of the $X_i; i \in 1...D$ is a univariate distribution
* Precision is the inverse of variance ($var = \sigma^2$) that is $\beta =\frac{1}{\sigma^2}$
* for each $t_n$, $\mu_n = w^T \times \Phi(x_n)$ the value we compare $t_n$ to.
  * Equation 3_12
* $\Phi$ is given the likehood function is $ln(p(t| w,\beta))$
# 141
* Section 1.2.4 (page 29 or 49/758) we already did the calculation (we have to minimize the sum of Squares)
* the Nabla operator is written by convention on one line, so the use of $\Phi(x_n)^T$
# 142
*  (MxN) x (NxM) = M x M 
*  $(M \times M)^{-1} \times (M \times N)$ gives $M \times N$ and **t** = $(t_1, ..., t_N)^T$ if of $N\times1$ dimension
  * which gives us a $M \times 1$ vector of $W_{ML}$ coefficients $(w_{ML_0}, ... , w_{ML_{M-1}})^T$
* A projection on $\phi_0, \phi_1, ...,\phi_{M-1}$ is a **Linear Combination** of those vectors
  * here through the parameters $w_0, w_1, ..., w_{M-1}$
# 144 paragraph 3.1.3
## Remember 
* $\phi$ stands for (see p 138-139) 
  * fixed preprocessing
  * feature extraction 
  * $\phi_j(x)$ where j $\in 1..M$ ($\phi_j$ function from space of dimension of X: D to 1 because y is a real)
    * and i for $x_i$ $\in 1 .. D$ (D dimension of vector x)
    * see equantions 3.1 and 3.2 p 138
* Equation 3.16 N is the number of points we are using for training (each point is of dimension D and M -1 <= D is the number of feature extractors)
 * for each point $x_i$, $y_i$ (i from 1 to N) is a linear combination of the feature extractors of a line (see equation 3.3 page 138)
   * the simple case is when $\phi_0(x_i) = 1$ and $\phi_j(x_i) = x_{i,j}$ the $j^{th}$ coordinate of $x_i$ j $\in 1..D$ 
     * see equation 3.1 p 138 and here M -1 = D $\eqsim$ M = D + 1
  * Equation 3.17 if $\Phi$ is square 
  * Equation 3.17 $\Phi^+$ is a M x N matrix when $\Phi$ was a N x M matrix: we have $\Phi^+ \times \Phi$ is a M x M identitity
    * I have to demonstrate that $\Phi^+ \times \Phi \times \Phi^+$ is $\Phi^+$ see [Moore Penrose definition on Wikipedia](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Definition)
    * whe can also see that if $\Phi$ square and invertible $\Phi^+ = \Phi^{-1}$ 
  * Equation 3.14 0 is a line  of M zeros
  * Equation 3.15 $W_{ML}$ is a column of M velues $w_0, w_1, ..., w_{M-1}$
    * passing from Line to column  $w^T \times \Phi \times \Phi^T$ becomes $\Phi^T \times \Phi \times w^T$
  * for going from  3.14 to 3.15 see [Formulations for Linear Regression on Wikipedia](https://en.wikipedia.org/wiki/Linear_least_squares#Formulations_for_Linear_Regression)
## Back to 144 
* Equation 3.23 is the last $\phi_n$ not $\phi_n^T$ see equation 3.13
  * no if we are considering here a column vector of $w_j$ values, j $\in 0..(M-1)$
# 145
* equation 3.28 comes from the fact that $w^T \times w$ when differentiated gives w so we add a $\lambda \times I$ term to the solution 
# 146 Multiple outputs
$W^T =\begin{bmatrix}
    w_{11} & w_{21} & w_{31} & \dots  & w_{M1} \\
    w_{12} & x_{22} & w_{32} & \dots  & w_{M2} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    w_{1K} & w_{2K} & w_{3K} & \dots  & w_{MK}
\end{bmatrix}
$
* Where $w_{ij}$ it the parameter for the $i^{th}$ component $\phi_i(x)$ (i $\in 1 \dots M$) for the $j^{th}$ output (j $\in 1 \dots K$)
* Equation *3.3.2* t vector of K dimention is the target variable for its K outputs
  * $W^T \times \phi(x)$ is a kind of mean value in the Gaussian distribution
* $T = \begin{matrix}
  t_1^T \\
  t_2^T \\
  \vdots \\
  t_N^T
\end{matrix}$ of dimension $N \times K$
* doing the same thing for $x_i$ we have: $X = \begin{matrix}
  x_1^T \\
  x_2^T \\
  \vdots \\
  x_N^T
\end{matrix}$ where X id of dimension $N \times dim(x_i)$
* Equation 3.33 $p(T|X) = \Pi_{i=1}^N (p(t_i|x_i))$ the log is then the $\Sigma_{i=1}^N$
# 147
* see equation 3.15 (page 142) $\Phi$ is a $N \times M$ matrix
  * we had indeed  $\Phi =\begin{bmatrix}
    \phi_0(x_1) & \phi_1(x_1) & \phi_2(x_1) & \dots  & \phi_{M-1}(x_1) \\
    \phi_0(x_2) & \phi_1(x_2) & \phi_2(x_2) & \dots  & \phi_{M-1}(x_2) \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    \phi_0(x_N) & \phi_1(x_N) & \phi_2(x_N) & \dots  & \phi_{M-1}(x_N)
\end{bmatrix}$
* $W_{ML}$ is of dimension $M \times K$
* equation 3.35 relates to $w_k, k \in 1...K$ as the $k^{th}$ column of $W_{ML}$ to be associated with the $k^{th}$ column of T
* p 94 we address to the exercise 2.34 (p ) to show that $\Sigma_{ML} =\Sigma_{n=1}^N(x_n - \mu_{ML})\times(x_n - \mu_{ML})^T$