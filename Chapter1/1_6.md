# p 50
* information -$log(p(x))) (1.92)
  * the les probable the event is the most value it has when it occurs
* Entropy - $\sum_x p(x) \times log(px)$
# P 51
* we apply [Stirling approximation (1.96)](https://en.wikipedia.org/wiki/Stirling%27s_approximation) to both $n_i$ and N which both become
  * important because $n_i/N$ is held constant
* formula 1.97 comes from the fact that $1 == \sum_i \frac{n_i}{N}$
## Annex E 
### p 706: 
* $\Delta g$ is orthogonal to the curve because of the first order Taylor expansion (They don't speak of the term of higher order, if they cancel the first order term, they infact become negligible when $\epsilon \rightarrow 0$)
  * when $\epsilon \rightarrow 0$ the $\epsilon$ vector thed to be parallel to the curve of x / g(x) = 0
### p 707 
* if $\Delta f$ was not normal to the surface (which is represented by a curve in E1 - two dimensional variables)
  * the x on the surface that maximize f(x) would be at another point on the surface (guided by $\Delta f$)
* so E3 is given by $\Delta_x L_x = 0$
  * with *$L_x = f + \lambda \times g$* 
* and the constraint g(x) = 0 is given by $\Delta_\lambda L = \frac{\partial L}{\partial \lambda} = 0$
   * still with *$L_x = f + \lambda \times g$* 
### p 708
* The concrete example $E_5$ is very interesting
  * figure E_2: $x_1^2 + x_2^2$ is the distance os point x from center 0
  * the more that distance is little the more f(x) is big (in any case less than 1)
### p 709 figure E3
* if g(x) > 0 any point $x_B$ in the yello surface (for which g(x) > 0) for which $\Delta_x f = 0$ makes the deal
  * it corresponds to $\Delta_x L_x = 0$ with $\lambda = 0 and g(x) > 0$
* if g(x) = 0 (red curve) then $\Delta_x f(x)$ must be opposite to $\Delta_x g(x)$ otherwise $x_A$ would go inside the orange surface and then g(x) > 0
  * if it was not exactly opposite to g(x) then another point on the curve would fit
  * it corresponds to $\Delta_x L_x = 0$ with $\lambda > 0 and g(x) = 0$
* see [the Stack Answer update to understand the sign question](https://math.stackexchange.com/questions/3891600/sign-of-lagrange-multiplier-with-inequality-constraints)
  * with the internal of a simple circle od radius 1 as the constraint and y=x as the y=f(x)
* followint he [StackOVerflox example with a simple circle 9 update](https://math.stackexchange.com/questions/3891600/sign-of-lagrange-multiplier-with-inequality-constraints), we can say:
  * The more I enter the surface (g(x) > 0) the more I diminish f when looking fo the max
    * To maximise I have to escape the domain of constraint (g(x) > 0)
  * The more I enter the surface (g(x) > 0) the more I augment f when looking for the min
# Back to page 51
* Formula 1.98 $p(x_i) < 1$ so $\log(p(x_i)) < 0 \implies \tilde{H} >> 0$
* Formula 1.99 back to Lagrange equation with the constraint $g(x_1, x_2, x_i, ...) = \Sigma_i p(x_i) -1 = 0$
  * and $f(x_1, x_2, x_i, ...) = - \Sigma_i p(x_i) \times \log(p(x_i))$
# p 52
* formula 1.101: tells there exists an $x_i$ somewhere between $i \times \Delta$ and $(i+1) \times \Delta$
* If in the formula 1.102 we omit $- \log(\Delta)$ we omit the fix part although that part tend to $+ \infty$ as $\Delta$ tehds to zero ???
# p 54
* Formula 1.108 comes from the calculcus of variations which I don't understand see Appendix D
## Appendix D 
### p 704
* Formula D6: if we compare
  * $y(x)$ with $y(x) + \epsilon(x)$  see figure D1
  * we compare $y^\prime(x)$ whidth $y^\prime(x) + \epsilon^\prime(x)$
* Formula D7 [intagration by parts Wikipedia](https://en.wikipedia.org/wiki/Integration_by_parts)
  * here $\eta(x)$ = $(v(x))$
  * and $\frac{\partial G}{\partial \prime{y}}$ = $u(x)$ 