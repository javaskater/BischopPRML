# 14
* formula p(X = $x_i$) = $\sum_{j=1}^L p(X=x_i, Y=y_j)$ a sum over the column $c_i$
  * marginal probability because obtained by maginalising or summing up over the other variables than X!
* p(Y=$y_j$|x=$x_i$) is the proportion of A/B
  * A is the total of Y equals $y_j$ (knowing X=$x_i$) = $n_{i,j}$
  * B is the value for considering only those instances in which X = $x_i$ which totals to $c_i = \sum_{j=1}^L n_{i,j}$ 
* p(Y=$y_j$|x=$x_i$) = $\frac{n_{i,j}}{\sum_{j=1}^L n_{i,j}}$ 
  * I only considers the cas X=$x_i$ (which happens ${\sum_{j=1}^L n_{i,j}}$ times) when calculating the P(Y=$y_j$) (which happens $n_{i,j}$ times)
* p(X,Y) joint probability $p(X=x_i, Y=y_j) = \frac{n_{i,j}}{N}$
* the sum rule gives the marginal probability
# 15
* a not so known property
* $p(X) = \sum_Y(P(X,Y))$ that the marginalisation we saw earlier
* p 14 we saw theBayes rule so $p(X) = \sum_Y(P(X|Y) \times P(Y))$
* Bayes states that $P(Y|X) = \frac{P(X|Y) \times P(Y)}{P(X)}$
  * so $\sum_Y P(Y|X) = \frac{P(X)}{P(X)} = 1$
  * The Book says *We can view the denominator in Bayesâ€™ theorem as being the normalization constant required to ensure that the sum of the conditional probability on the left-hand side of (1.12) over all values of Y equals one.*

# 17
* Prior or posterior probabilities !