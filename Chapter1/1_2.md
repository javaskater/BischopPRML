# 14
* formula p(X = $x_i$) = $\sum_{j=1}^L p(X=x_i, Y=y_j)$ a sum over the column $c_i$
  * marginal probability because obtained by maginalising or summing up over the other variables than X!
* p(Y=$y_j$|x=$x_i$) is the proportion of A/B
  * A is the total of Y equals $y_j$ (knowing X=$x_i$) = $n_{i,j}$
  * B is the value for considering only those instances in which X = $x_i$ which totals to $c_i = \sum_{j=1}^L n_{i,j}$ 
* p(Y=$y_j$|x=$x_i$) = $\frac{n_{i,j}}{\sum_{j=1}^L n_{i,j}}$ 
  * I only considers the cas X=$x_i$ (which happens ${\sum_{j=1}^L n_{i,j}}$ times) when calculating the P(Y=$y_j$) (which happens $n_{i,j}$ times)
* p(X,Y) joint probability $p(X=x_i, Y=y_j) = \frac{n_{i,j}}{N}$
* the sum rule gives the marginal probability
# 15
* a not so known property
* $p(X) = \sum_Y(P(X,Y))$ that the marginalisation we saw earlier
* p 14 we saw theBayes rule so $p(X) = \sum_Y(P(X|Y) \times P(Y))$
* Bayes states that $P(Y|X) = \frac{P(X|Y) \times P(Y)}{P(X)}$
  * so $\sum_Y P(Y|X) = \frac{P(X)}{P(X)} = 1$
  * The Book says *We can view the denominator in Bayesâ€™ theorem as being the normalization constant required to ensure that the sum of the conditional probability on the left-hand side of (1.12) over all values of Y equals one.*

# 17
* Prior or posterior probabilities !
# 20
* from 1.34 to 1.35 
  * only if all points have the same probability to occur (1/N)?
  * no the points are drawn from the probability distribution p(x)!!!
* 1.37 the average of the function f(x,y) with respect to the distribution of x - p(x).
  * it is a function of y
  * $g(y) = E_x[f|y]$ = $\sum_{x}p(x|y)f(x)$
* [Covariance formula and independent variables](https://en.wikipedia.org/wiki/Covariance)
  * independent variables means $E(X \times Y) = E(X) \times E(Y)$
  * for vector variables X and Y are column vectors !!! [same link](https://en.wikipedia.org/wiki/Covariance) as before.
# 22
* prior: $p(w)$
* likehood function $p(D|w)$
* posterior probability: $p(w|D)$
* The posterior probaility is a valid probability dendity see equation 1.45 !!!
# 23
> maximizes the likelihood function p(D|w). This corresponds to
> choosing the value of w for which the probability of the observed data set is maximized
* maximizing the likehood is minimizing the error $-log$
* frequentist error bars (L? bars using [bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)))
* My question is how we calculate $-log(p(D|w))$ over a dample of N observations

# 24
* [Gaussian or normal distributions](https://en.wikipedia.org/wiki/Normal_distribution) are the same thing...
  * $\sigma$ is the standard deviation
  * calculated from the variance or $\sigma^2$
  * $\beta = \frac{1}{\sigma^2}$ is called the precision
    * the more the variance is important the lower the precision 
  * The mean $\int_{-\infty}^{+\infty} p(x) f(x) dx$ with $f(x) = x$ and $p(x) = N(x|\mu, \sigma^2)$is the first order moment $E[x]$
    * the second order moment $E[x^2]$ is calculated with $f(x) = x^2$
  * remember that $var(x) = E[x^2] - E[x]^2 = \sigma^2$  
# 25 
* [Multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)
  * $\Sigma$ is a $D \times D$ Matrix
  * $|\Sigma|$ is the determinant of matrix $\Sigma$
  * [$\Sigma$ is the covariance matrix](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)
    * $\Sigma_{i,j} = E[(X_i - E(X_i)) \times (X_j - E(X_j))]$ is $cov(x_i, y_j)$ 
  * $Q = \Sigma^{-1}$ is the precision matrix!!!
# 26
* [definition of a joint and marginal probability](https://machinelearningmastery.com/joint-marginal-and-conditional-probability-for-machine-learning/)
* iid stands for _independant and identically distributed
* [maximum likehood for normal distribution](https://www.statlect.com/fundamentals-of-statistics/normal-distribution-maximum-likelihood)
  * [more genrally the bassics maximal likehood estimation](https://www.statlect.com/fundamentals-of-statistics/maximum-likelihood)
  * this [video shows the maths in a practical waybut not the proofs](https://www.youtube.com/watch?v=Dn6b9fCIUpM)
> it would seem more natural to maximize the probability of the parameters given the data,
> not the probability of the data given the parameters.
> In fact, these two criteria are related, as we shall discuss in the context of curve fitting.
# 27 
* [calculations shown also in the video](https://www.youtube.com/watch?v=Dn6b9fCIUpM)
* that [Jupyter Notebook](https://github.com/vagmcs/PRML/blob/master/notebooks/ch1_introduction.ipynb) shows it !!!
```python
>>> import numpy as np
>>> mu_space = np.linspace(-1, 1, 10)
>>> mu_space
array([-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,
        0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ])
>>> var_space = np.linspace(0, 2, 10)
>>> var_space
array([0.        , 0.22222222, 0.44444444, 0.66666667, 0.88888889,
       1.11111111, 1.33333333, 1.55555556, 1.77777778, 2.        ])
>>> mu_mesh, var_mesh = np.meshgrid(mu_space, var_space)
>>> mu_mesh
array([[-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,
         0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ],
       [-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,
         0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ],
       [-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,
         0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ],
       [-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,
         0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ],
       [-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,
         0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ],
       [-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,
         0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ],
       [-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,
         0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ],
       [-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,
         0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ],
       [-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,
         0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ],
       [-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,
         0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ]])
>>> var_mesh
array([[0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ],
       [0.22222222, 0.22222222, 0.22222222, 0.22222222, 0.22222222,
        0.22222222, 0.22222222, 0.22222222, 0.22222222, 0.22222222],
       [0.44444444, 0.44444444, 0.44444444, 0.44444444, 0.44444444,
        0.44444444, 0.44444444, 0.44444444, 0.44444444, 0.44444444],
       [0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.66666667,
        0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.66666667],
       [0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889,
        0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889],
       [1.11111111, 1.11111111, 1.11111111, 1.11111111, 1.11111111,
        1.11111111, 1.11111111, 1.11111111, 1.11111111, 1.11111111],
       [1.33333333, 1.33333333, 1.33333333, 1.33333333, 1.33333333,
        1.33333333, 1.33333333, 1.33333333, 1.33333333, 1.33333333],
       [1.55555556, 1.55555556, 1.55555556, 1.55555556, 1.55555556,
        1.55555556, 1.55555556, 1.55555556, 1.55555556, 1.55555556],
       [1.77777778, 1.77777778, 1.77777778, 1.77777778, 1.77777778,
        1.77777778, 1.77777778, 1.77777778, 1.77777778, 1.77777778],
       [2.        , 2.        , 2.        , 2.        , 2.        ,
        2.        , 2.        , 2.        , 2.        , 2.        ]])
```
* in that code it uses an interesting function:
```python
import math
import numpy as np
import matplotlib.pyplot as plt
from prml.distribution import Gaussian, MultivariateGaussian
####################################
for i, mu in enumerate(mu_space):
    for j, var in enumerate(var_space):
        ll[i, j] = Gaussian(mu, var).likelihood_iid(x_sample_data)
```
* in the [demo for 1.58](https://github.com/vagmcs/PRML/blob/ff0ea6b45dee56b644886db45f645441d3142d76/notebooks/ch1_introduction.ipynb)
* following that [the average of the product of two independent random variable is the product of the average](https://muchomas.lassp.cornell.edu/8.04/Lecs/lec_statistics/node13.html) 
* $(\sum_1^N(x_n))^2 = \sum_1^N(x_n^2) + \sum_{i \neq j} (x_i \times x_j)$
  * the first part has N elements and for each of the $x_n^2$ the average is $\mu^2 + \sigma^2$
  * the second part has $N \times (N - 1)$ elements and for each of the $(x_i \times x_j)$ the average is $\mu^2$ (because $x_i$ and $x_j$ are independant)
# 28
* $t_n$ the expected ordinate of abscissa value $x_n$
* $y(x,w)$ the calculated ordinate of abscissa value $x_n$
* $\beta = \frac{1}{\sigma^2}$ is the so-called precision int the Gaussian distributions !!!
# p 29 
* figure 1.16 is interesting on the y-axis we get in blue the probaility that taht y value is taken for that $x_0$