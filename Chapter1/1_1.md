# 5
* $y(x,w) = \sum_{j=0}^M w_j \times x^j$
## linear models
* because the polynomial is linear in term of the parameters $w_1, w_2, ..., w_m$
* x and w are of the same dimension: _M_
## we train w on $x_n$ not $\hat{x}_n$
* E is the error not the mean function $\mathbb{E}$
* E(w) = $\frac{1}{2} \times \sum_{n=1}^N \left\{y(x_n,w) -t\right\}^2$
  * w is the polynomial with coefficients $( w_0, w_1, ..., w_{M-1})$
  * N is the size of the train dataset
# 6
* solution which minimizes the error is denoted __y(x,w*)__
# 7
* RMS (Root Mean Square) Error is given by $E_{RMS}=\sqrt{2 \times E(w)/N}$
  * N to compare datasets of different sizes on equal footing
  * $\sqrt{}$ to ensure $E_{RMS}$ is of the same scale (same unit) as t
# 9
* *Another way to say this is that the larger the data set, the more complex (in other words more flexible) the model that we can afford to fit to the data.*
* *in a Bayesian model the effective number of parameters adapts automatically to the size of the data set.*
# 11
* the RMS Error comes form the original error without the weights penalty !!!